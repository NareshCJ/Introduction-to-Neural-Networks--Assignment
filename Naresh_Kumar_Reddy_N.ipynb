{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Assignment_Naresh_Kumar_Reddy_N.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNoyneaKZQk8",
        "colab_type": "text"
      },
      "source": [
        "In this assignment, you'll implement an L-layered deep neural network and train it on the MNIST dataset. The MNIST dataset contains scanned images of handwritten digits, along with their correct classification labels (between 0-9). MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_XbDOtNZQlF",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wagMGikAZQlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxspN6Q_bHco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "6d2a54a4-98d1-47c3-8638-446d84038471"
      },
      "source": [
        "# If we are runing from colab data set \n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSEBA_FWZQlr",
        "colab_type": "text"
      },
      "source": [
        "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq7Y-DD8ZQlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we are runing from colab data set \n",
        "# def load_data():\n",
        "#     f = gzip.open('/content/drive/My Drive/mnist.pkl.gz', 'rb')\n",
        "#     f.seek(0)\n",
        "#     training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "#     f.close()\n",
        "#     return (training_data, validation_data, test_data)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5n0r5VEOoxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If we are not runing from colab data set \n",
        "def load_data():\n",
        "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
        "    f.seek(0)\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PzO2at_ZQl_",
        "colab_type": "text"
      },
      "source": [
        "Let's see how the data looks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWiumPDpZQmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data, validation_data, test_data = load_data()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGf2kL8PZQmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "fd1f6c58-21d4-4f6e-c1c7-88249cab3606"
      },
      "source": [
        "training_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
              " array([5, 0, 4, ..., 8, 4, 8]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJcqRwPVZQmh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ebd44a9f-19f6-4492-f9a4-86d522331f6a"
      },
      "source": [
        "# shape of data\n",
        "print(training_data[0].shape)\n",
        "print(training_data[1].shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV8RlC84ZQm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "328d6c5e-eda0-4d71-a900-0a28b63d860f"
      },
      "source": [
        "print(\"The feature dataset is:\" + str(training_data[0]))\n",
        "print(\"The target dataset is:\" + str(training_data[1]))\n",
        "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
        "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "The target dataset is:[5 0 4 ... 8 4 8]\n",
            "The number of examples in the training dataset is:50000\n",
            "The number of points in a single input is:784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmRAIxS3ZQnH",
        "colab_type": "text"
      },
      "source": [
        "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0OGby2RZQnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(j):\n",
        "    # input is the target dataset of shape (m,) where m is the number of data points\n",
        "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
        "    # Look at the next block of code for a better understanding of one hot encoding\n",
        "    n = j.shape[0]\n",
        "    new_array = np.zeros((10, n))\n",
        "    index = 0\n",
        "    for res in j:\n",
        "        new_array[res][index] = 1.0\n",
        "        index = index + 1\n",
        "    return new_array"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpeoAJ8QZQnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "c5920583-6d67-4a0d-bd69-e6afe343ff17"
      },
      "source": [
        "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "print(data.shape)\n",
        "one_hot(data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWJSmJVRZQnf",
        "colab_type": "text"
      },
      "source": [
        "The following function data_wrapper() will convert the dataset into the desired shape and also convert the ground truth labels to one_hot matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezA4Sj1ZQnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    \n",
        "    training_inputs = np.array(tr_d[0][:]).T\n",
        "    training_results = np.array(tr_d[1][:])\n",
        "    train_set_y = one_hot(training_results)\n",
        "    \n",
        "    validation_inputs = np.array(va_d[0][:]).T\n",
        "    validation_results = np.array(va_d[1][:])\n",
        "    validation_set_y = one_hot(validation_results)\n",
        "    \n",
        "    test_inputs = np.array(te_d[0][:]).T\n",
        "    test_results = np.array(te_d[1][:])\n",
        "    test_set_y = one_hot(test_results)\n",
        "    \n",
        "    return (training_inputs, train_set_y, test_inputs, test_set_y)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNXMyXvVZQnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1WtSpwSZQn_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "d4bef5df-8320-45ab-a4e5-a3e4fe07d598"
      },
      "source": [
        "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_set_x shape: (784, 50000)\n",
            "train_set_y shape: (10, 50000)\n",
            "test_set_x shape: (784, 10000)\n",
            "test_set_y shape: (10, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIcWfL7zZQoQ",
        "colab_type": "text"
      },
      "source": [
        "We can see that the data_wrapper has converted the training and validation data into numpy array of desired shapes. Let's convert the actual labels into a dataframe to see if the one hot conversions are correct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qicdglo-ZQoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = pd.DataFrame(train_set_y)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEVen8DNZQol",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "50528756-4cda-487c-91e6-5aee9a5f7572"
      },
      "source": [
        "print(\"The target dataset is:\" + str(training_data[1]))\n",
        "print(\"The one hot encoding dataset is:\")\n",
        "y"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The target dataset is:[5 0 4 ... 8 4 8]\n",
            "The one hot encoding dataset is:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>49960</th>\n",
              "      <th>49961</th>\n",
              "      <th>49962</th>\n",
              "      <th>49963</th>\n",
              "      <th>49964</th>\n",
              "      <th>49965</th>\n",
              "      <th>49966</th>\n",
              "      <th>49967</th>\n",
              "      <th>49968</th>\n",
              "      <th>49969</th>\n",
              "      <th>49970</th>\n",
              "      <th>49971</th>\n",
              "      <th>49972</th>\n",
              "      <th>49973</th>\n",
              "      <th>49974</th>\n",
              "      <th>49975</th>\n",
              "      <th>49976</th>\n",
              "      <th>49977</th>\n",
              "      <th>49978</th>\n",
              "      <th>49979</th>\n",
              "      <th>49980</th>\n",
              "      <th>49981</th>\n",
              "      <th>49982</th>\n",
              "      <th>49983</th>\n",
              "      <th>49984</th>\n",
              "      <th>49985</th>\n",
              "      <th>49986</th>\n",
              "      <th>49987</th>\n",
              "      <th>49988</th>\n",
              "      <th>49989</th>\n",
              "      <th>49990</th>\n",
              "      <th>49991</th>\n",
              "      <th>49992</th>\n",
              "      <th>49993</th>\n",
              "      <th>49994</th>\n",
              "      <th>49995</th>\n",
              "      <th>49996</th>\n",
              "      <th>49997</th>\n",
              "      <th>49998</th>\n",
              "      <th>49999</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 50000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0      1      2      3      4      ...  49995  49996  49997  49998  49999\n",
              "0    0.0    1.0    0.0    0.0    0.0  ...    0.0    1.0    0.0    0.0    0.0\n",
              "1    0.0    0.0    0.0    1.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "2    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "3    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "4    0.0    0.0    1.0    0.0    0.0  ...    0.0    0.0    0.0    1.0    0.0\n",
              "5    1.0    0.0    0.0    0.0    0.0  ...    1.0    0.0    0.0    0.0    0.0\n",
              "6    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "7    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "8    0.0    0.0    0.0    0.0    0.0  ...    0.0    0.0    1.0    0.0    1.0\n",
              "9    0.0    0.0    0.0    0.0    1.0  ...    0.0    0.0    0.0    0.0    0.0\n",
              "\n",
              "[10 rows x 50000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqjk98TmZQow",
        "colab_type": "text"
      },
      "source": [
        "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-bY8CRMZQo0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "65cdeaa4-54ce-4f1d-ec1e-3ca2134f8e19"
      },
      "source": [
        "index  = 1000\n",
        "k = train_set_x[:,index]\n",
        "k = k.reshape((28, 28))\n",
        "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
        "plt.imshow(k, cmap='gray')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fabdb59ab38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb5XbQSKZQpA",
        "colab_type": "text"
      },
      "source": [
        "# Feedforward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPk1syN-ZQpC",
        "colab_type": "text"
      },
      "source": [
        "### sigmoid\n",
        "This is one of the activation functions. It takes the cumulative input to the layer, the matrix **Z**, as the input. Upon application of the **`sigmoid`** function, the output matrix **H** is calculated. Also, **Z** is stored as the variable **sigmoid_memory** since it will be later used in backpropagation.You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here in the following way. The exponential gets applied to all the elements of Z."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgE5TNC1ZQpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(Z):\n",
        "    \n",
        "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
        "    # sigmoid_memory is stored as it is used later on in backpropagation\n",
        "    \n",
        "    H = 1/(1+np.exp(-Z))\n",
        "    sigmoid_memory = Z\n",
        "    \n",
        "    return H, sigmoid_memory"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyZhzj5VZQpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "18e61145-ecec-4a22-9623-bc4bc877d60a"
      },
      "source": [
        "Z = np.arange(8).reshape(4,2)\n",
        "print (\"sigmoid(Z) = \" + str(sigmoid(Z)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid(Z) = (array([[0.5       , 0.73105858],\n",
            "       [0.88079708, 0.95257413],\n",
            "       [0.98201379, 0.99330715],\n",
            "       [0.99752738, 0.99908895]]), array([[0, 1],\n",
            "       [2, 3],\n",
            "       [4, 5],\n",
            "       [6, 7]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us2XP3KeZQpW",
        "colab_type": "text"
      },
      "source": [
        "### relu\n",
        "This is one of the activation functions. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`relu`** function, matrix **H** which is the output matrix is calculated. Also, **Z** is stored as **relu_memory** which will be later used in backpropagation. You use _[np.maximum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.maximum.html)_ here in the following way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysRvC3_sZQpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
        "    # relu_memory is stored as it is used later on in backpropagation\n",
        "    \n",
        "    H = np.maximum(0,Z)\n",
        "    \n",
        "    assert(H.shape == Z.shape)\n",
        "    \n",
        "    relu_memory = Z \n",
        "    return H, relu_memory"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X31ictr9ZQph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5b552704-3578-47a1-bf3f-2b5d724ed9de"
      },
      "source": [
        "Z = np.array([1, 3, -1, -4, -5, 7, 9, 18]).reshape(4,2)\n",
        "print (\"relu(Z) = \" + str(relu(Z)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "relu(Z) = (array([[ 1,  3],\n",
            "       [ 0,  0],\n",
            "       [ 0,  7],\n",
            "       [ 9, 18]]), array([[ 1,  3],\n",
            "       [-1, -4],\n",
            "       [-5,  7],\n",
            "       [ 9, 18]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eZvqgd3ZQpq",
        "colab_type": "text"
      },
      "source": [
        "### softmax\n",
        "This is the activation of the last layer. It takes the cumulative input to the layer, matrix **Z** as the input. Upon application of the **`softmax`** function, the output matrix **H** is calculated. Also, **Z** is stored as **softmax_memory** which will be later used in backpropagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ and _[np.sum()](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.sum.html)_ here in the following way. The exponential gets applied to all the elements of Z."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck6njYEkZQpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(Z):\n",
        "    # Z is numpy array of shape (n, m) where n is number of neurons in the layer and m is the number of samples \n",
        "    # softmax_memory is stored as it is used later on in backpropagation\n",
        "   \n",
        "    Z_exp = np.exp(Z)\n",
        "\n",
        "    Z_sum = np.sum(Z_exp,axis = 0, keepdims = True)\n",
        "    \n",
        "    H = Z_exp/Z_sum  #normalising step\n",
        "    softmax_memory = Z\n",
        "    \n",
        "    return H, softmax_memory"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnZiIFg2ZQp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z = np.array([[11,19,10], [12, 21, 23]])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynUiJy2NZQqB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c0352dc8-4e60-413b-eedb-16e7efa383f6"
      },
      "source": [
        "#Z = np.array(np.arange(30)).reshape(10,3)\n",
        "H, softmax_memory = softmax(Z)\n",
        "print(H)\n",
        "print(softmax_memory)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.68941421e-01 1.19202922e-01 2.26032430e-06]\n",
            " [7.31058579e-01 8.80797078e-01 9.99997740e-01]]\n",
            "[[11 19 10]\n",
            " [12 21 23]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExlb5qrZQqO",
        "colab_type": "text"
      },
      "source": [
        "### initialize_parameters\n",
        "Let's now create a function **`initialize_parameters`** which initializes the weights and biases of the various layers. One way to initialise is to set all the parameters to 0. This is not a considered a good strategy as all the neurons will behave the same way and it'll defeat the purpose of deep networks. Hence, we initialize the weights randomly to very small values but not zeros. The biases are initialized to 0. Note that the **`initialize_parameters`** function initializes the parameters for all the layers in one `for` loop. \n",
        "\n",
        "The inputs to this function is a list named `dimensions`. The length of the list is the number layers in the network + 1 (the plus one is for the input layer, rest are hidden + output). The first element of this list is the dimensionality or length of the input (784 for the MNIST dataset). The rest of the list contains the number of neurons in the corresponding (hidden and output) layers.\n",
        "\n",
        "For example `dimensions = [784, 3, 7, 10]` specifies a network for the MNIST dataset with two hidden layers and a 10-dimensional softmax output.\n",
        "\n",
        "Also, notice that the parameters are returned in a dictionary. This will help you in implementing the feedforward through the layer and the backprop throught the layer at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yChe4h-oZQqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(dimensions):\n",
        "\n",
        "    # dimensions is a list containing the number of neuron in each layer in the network\n",
        "    # It returns parameters which is a python dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "\n",
        "    np.random.seed(2)\n",
        "    parameters = {}\n",
        "    L = len(dimensions)            # number of layers in the network + 1\n",
        "\n",
        "    for l in range(1, L): \n",
        "        parameters['W' + str(l)] = np.random.randn(dimensions[l], dimensions[l-1]) * 0.1\n",
        "        parameters['b' + str(l)] = np.zeros((dimensions[l], 1)) \n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (dimensions[l], dimensions[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (dimensions[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "syrXuVMUZQqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "2be119d2-5a15-48b9-c355-b9a091bceea0"
      },
      "source": [
        "dimensions  = [784, 3,7,10]\n",
        "parameters = initialize_parameters(dimensions)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
        "# print(\"b3 = \" + str(parameters[\"b3\"]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = [[-0.04167578 -0.00562668 -0.21361961 ... -0.06168445  0.03213358\n",
            "  -0.09464469]\n",
            " [-0.05301394 -0.1259207   0.16775441 ... -0.03284246 -0.05623108\n",
            "   0.01179136]\n",
            " [ 0.07386378 -0.15872956  0.01532001 ... -0.08428557  0.10040469\n",
            "   0.00545832]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[ 0.06650944 -0.19626047  0.2112715 ]\n",
            " [-0.28074571 -0.13967752  0.02641189]\n",
            " [ 0.10925169  0.06646016  0.08565535]\n",
            " [-0.11058228  0.03715795  0.13440124]\n",
            " [-0.16421272 -0.1153127   0.02013163]\n",
            " [ 0.13985659  0.07228733 -0.10717236]\n",
            " [-0.05673344 -0.03663499 -0.15460347]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJyWwIozZQqk",
        "colab_type": "text"
      },
      "source": [
        "### layer_forward\n",
        "\n",
        "The function **`layer_forward`** implements the forward propagation for a certain layer 'l'. It calculates the cumulative input into the layer **Z** and uses it to calculate the output of the layer **H**. It takes **H_prev, W, b and the activation function** as inputs and stores the **linear_memory, activation_memory** in the variable **memory** which will be used later in backpropagation. \n",
        "\n",
        "<br> You have to first calculate the **Z**(using the forward propagation equation), **linear_memory**(H_prev, W, b) and then calculate **H, activation_memory**(Z) by applying activation functions - **`sigmoid`**, **`relu`** and **`softmax`** on **Z**.\n",
        "\n",
        "<br> Note that $$H^{L-1}$$ is referred here as H_prev. You might want to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_ to carry out the matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2EntGsMZQqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graded\n",
        "\n",
        "def layer_forward(H_prev, W, b, activation = 'relu'):\n",
        "\n",
        "    # H_prev is of shape (size of previous layer, number of examples)\n",
        "    # W is weights matrix of shape (size of current layer, size of previous layer)\n",
        "    # b is bias vector of shape (size of the current layer, 1)\n",
        "    # activation is the activation to be used for forward propagation : \"softmax\", \"relu\", \"sigmoid\"\n",
        "\n",
        "    # H is the output of the activation function \n",
        "    # memory is a python dictionary containing \"linear_memory\" and \"activation_memory\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z = np.dot(W,H_prev) + b #write your code here\n",
        "        linear_memory = (H_prev, W, b)\n",
        "        H, activation_memory = sigmoid(Z) #write your code here\n",
        " \n",
        "    elif activation == \"softmax\":\n",
        "        Z = np.dot(W,H_prev) + b  #write your code here\n",
        "        linear_memory = (H_prev, W, b)\n",
        "        H, activation_memory = softmax(Z)   #write your code here\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z = np.dot(W,H_prev) + b  #write your code here\n",
        "        linear_memory = (H_prev, W, b)\n",
        "        H, activation_memory =  relu(Z) #write your code here\n",
        "        \n",
        "    assert (H.shape == (W.shape[0], H_prev.shape[1]))\n",
        "    memory = (linear_memory, activation_memory)\n",
        "\n",
        "    return H, memory"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9uf_fc_ZQqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f28cd729-74fb-4080-eeda-53ab30cf3e67"
      },
      "source": [
        "# verify\n",
        "# l-1 has two neurons, l has three, m = 5\n",
        "# H_prev is (l-1, m)\n",
        "# W is (l, l-1)\n",
        "# b is (l, 1)\n",
        "# H should be (l, m)\n",
        "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
        "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
        "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
        "\n",
        "H = layer_forward(H_prev, W_sample, b_sample, activation=\"sigmoid\")[0]\n",
        "H"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 1.        , 1.        , 1.        , 1.        ],\n",
              "       [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],\n",
              "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS7OlC2ZZQq5",
        "colab_type": "text"
      },
      "source": [
        "You should get:<br>\n",
        "    array([[1.        , 1.        , 1.        , 1.        , 1.        ],<br>\n",
        "      [0.99908895, 0.99330715, 0.99999969, 1.        , 0.99987661],<br>\n",
        "       [0.73105858, 0.5       , 0.99330715, 0.9999546 , 0.88079708]])\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XihkYIL3ZQq9",
        "colab_type": "text"
      },
      "source": [
        "### L_layer_forward\n",
        "**`L_layer_forward`** performs one forward pass through the whole network for all the training samples (note that we are feeding all training examples in one single batch). Use the **`layer_forward`** you have created above here to perform the feedforward for layers 1 to 'L-1' in the for loop with the activation **`relu`**. The last layer having a different activation **`softmax`** is calculated outside the loop. Notice that the **memory** is appended to **memories** for all the layers. These will be used in the backward order during backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmR6U85lZQq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graded\n",
        "\n",
        "def L_layer_forward(X, parameters):\n",
        "\n",
        "    # X is input data of shape (input size, number of examples)\n",
        "    # parameters is output of initialize_parameters()\n",
        "    \n",
        "    # HL is the last layer's post-activation value\n",
        "    # memories is the list of memory containing (for a relu activation, for example):\n",
        "    # - every memory of relu forward (there are L-1 of them, indexed from 1 to L-1), \n",
        "    # - the memory of softmax forward (there is one, indexed L) \n",
        "\n",
        "    memories = []\n",
        "    H = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement relu layer (L-1) times as the Lth layer is the softmax layer\n",
        "    for l in range(1, L):\n",
        "        H_prev = H #write your code here \n",
        "\n",
        "        W=parameters['W' + str(l)]\n",
        "        b=parameters['b' + str(l)]\n",
        "        \n",
        "        H, memory = layer_forward(H_prev,W,b, activation='relu') #write your code here\n",
        "        \n",
        "        memories.append(memory)\n",
        "    \n",
        "    # Implement the final softmax layer\n",
        "    # HL here is the final prediction P as specified in the lectures\n",
        "    HL, memory = layer_forward(H,parameters[ 'W' + str(L) ],parameters[ 'b' +str(L) ], activation='softmax') #write your code here\n",
        "    \n",
        "    memories.append(memory)\n",
        "\n",
        "    assert(HL.shape == (10, X.shape[1]))\n",
        "            \n",
        "    return HL, memories"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP48cwrOZQre",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "51264d20-ba2e-40a4-c896-f245bf3a7e55"
      },
      "source": [
        "# verify\n",
        "# X is (784, 10)\n",
        "# parameters is a dict\n",
        "# HL should be (10, 10)\n",
        "x_sample = train_set_x[:, 10:20]\n",
        "print(x_sample.shape)\n",
        "HL = L_layer_forward(x_sample, parameters=parameters)[0]\n",
        "print(HL[:, :5])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 10)\n",
            "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]\n",
            " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]\n",
            " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]\n",
            " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]\n",
            " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]\n",
            " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]\n",
            " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]\n",
            " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]\n",
            " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]\n",
            " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx-vEa9eZQrp",
        "colab_type": "text"
      },
      "source": [
        "You should get:\n",
        "\n",
        "(784, 10)<br>\n",
        "[[0.10106734 0.10045152 0.09927757 0.10216656 0.1       ]<br>\n",
        " [0.10567625 0.10230873 0.10170271 0.11250099 0.1       ]<br>\n",
        " [0.09824287 0.0992886  0.09967128 0.09609693 0.1       ]<br>\n",
        " [0.10028288 0.10013048 0.09998149 0.10046076 0.1       ]<br>\n",
        " [0.09883601 0.09953443 0.09931419 0.097355   0.1       ]<br>\n",
        " [0.10668575 0.10270912 0.10180736 0.11483609 0.1       ]<br>\n",
        " [0.09832513 0.09932275 0.09954792 0.09627089 0.1       ]<br>\n",
        " [0.09747092 0.09896735 0.0995387  0.09447277 0.1       ]<br>\n",
        " [0.09489069 0.09788255 0.09929998 0.08915178 0.1       ]<br>\n",
        " [0.09852217 0.09940447 0.09985881 0.09668824 0.1       ]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxgTJaeEZQru",
        "colab_type": "text"
      },
      "source": [
        "# Loss\n",
        "\n",
        "### compute_loss\n",
        "The next step is to compute the loss function after every forward pass to keep checking whether it is decreasing with training.<br> **`compute_loss`** here calculates the cross-entropy loss. You may want to use _[np.log()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log.html)_, _[np.multiply()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.multiply.html)_ here. Do not forget that it is the average loss across all the data points in the batch. It takes the output of the last layer **HL** and the ground truth label **Y** as input and returns the **loss**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB0fChNmZQrx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graded\n",
        "\n",
        "def compute_loss(HL, Y):\n",
        "\n",
        "\n",
        "    # HL is probability matrix of shape (10, number of examples)\n",
        "    # Y is true \"label\" vector shape (10, number of examples)\n",
        "\n",
        "    # loss is the cross-entropy loss\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    loss = -(1./m) * (np.sum(np.multiply(Y,np.log(HL))))#write your code here, use (1./m) and not (1/m)\n",
        "    \n",
        "    loss = np.squeeze(loss)      # To make sure that the loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(loss.shape == ())\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLm4WryLZQr8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "1d4c1184-ac9a-41ab-852f-1cdc543b7a47"
      },
      "source": [
        "# sample\n",
        "# HL is (10, 5), Y is (10, 5)\n",
        "np.random.seed(2)\n",
        "HL_sample = np.random.rand(10,5)\n",
        "Y_sample = train_set_y[:, 10:15]\n",
        "print(HL_sample)\n",
        "print(Y_sample)\n",
        "\n",
        "print(compute_loss(HL_sample, Y_sample))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]\n",
            " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]\n",
            " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]\n",
            " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]\n",
            " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]\n",
            " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]\n",
            " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]\n",
            " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]\n",
            " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]\n",
            " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]\n",
            "[[0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [1. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "0.8964600261334037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnPpIVOWZQsE",
        "colab_type": "text"
      },
      "source": [
        "You should get:<br>\n",
        "    \n",
        "[[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678 ]<br>\n",
        " [0.33033482 0.20464863 0.61927097 0.29965467 0.26682728]<br>\n",
        " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987]<br>\n",
        " [0.78533515 0.85397529 0.49423684 0.84656149 0.07964548]<br>\n",
        " [0.50524609 0.0652865  0.42812233 0.09653092 0.12715997]<br>\n",
        " [0.59674531 0.226012   0.10694568 0.22030621 0.34982629]<br>\n",
        " [0.46778748 0.20174323 0.64040673 0.48306984 0.50523672]<br>\n",
        " [0.38689265 0.79363745 0.58000418 0.1622986  0.70075235]<br>\n",
        " [0.96455108 0.50000836 0.88952006 0.34161365 0.56714413]<br>\n",
        " [0.42754596 0.43674726 0.77655918 0.53560417 0.95374223]]<br>\n",
        "[[0. 0. 0. 0. 0.]<br>\n",
        " [0. 0. 0. 0. 1.]<br>\n",
        " [0. 0. 0. 0. 0.]<br>\n",
        " [1. 0. 1. 0. 0.]<br>\n",
        " [0. 0. 0. 0. 0.]<br>\n",
        " [0. 1. 0. 0. 0.]<br>\n",
        " [0. 0. 0. 1. 0.]<br>\n",
        " [0. 0. 0. 0. 0.]<br>\n",
        " [0. 0. 0. 0. 0.]<br>\n",
        " [0. 0. 0. 0. 0.]]<br>\n",
        "0.8964600261334037"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VezfgQgZQsF",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "Let's now get to the next step - backpropagation. Let's start with sigmoid_backward.\n",
        "\n",
        "### sigmoid-backward\n",
        "You might remember that we had created **`sigmoid`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **sigmoid_memory** as input. **sigmoid_memory** is the **Z** which we had calculated during forward propagation. You use _[np.exp()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html)_ here the following way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckkJ5Y5OZQsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dH, sigmoid_memory):\n",
        "    \n",
        "    # Implement the backpropagation of a sigmoid function\n",
        "    # dH is gradient of the sigmoid activated activation of shape same as H or Z in the same layer    \n",
        "    # sigmoid_memory is the memory stored in the sigmoid(Z) calculation\n",
        "    \n",
        "    Z = sigmoid_memory\n",
        "    \n",
        "    H = 1/(1+np.exp(-Z))\n",
        "    dZ = dH * H * (1-H)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYH1w0noZQsP",
        "colab_type": "text"
      },
      "source": [
        "### relu-backward\n",
        "You might remember that we had created **`relu`** function that calculated the activation for forward propagation. Now, we need the activation backward, which helps in calculating **dZ** from **dH**. Notice that it takes input **dH** and **relu_memory** as input. **relu_memory** is the **Z** which we calculated uring forward propagation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjjc1RhpZQsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dH, relu_memory):\n",
        "    \n",
        "    # Implement the backpropagation of a relu function\n",
        "    # dH is gradient of the relu activated activation of shape same as H or Z in the same layer    \n",
        "    # relu_memory is the memory stored in the sigmoid(Z) calculation\n",
        "    \n",
        "    Z = relu_memory\n",
        "    dZ = np.array(dH, copy=True) # dZ will be the same as dA wherever the elements of A weren't 0\n",
        "    \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZWKkt2-ZQsl",
        "colab_type": "text"
      },
      "source": [
        "### layer_backward\n",
        "\n",
        "**`layer_backward`** is a complimentary function of **`layer_forward`**. Like **`layer_forward`** calculates **H** using **W**, **H_prev** and **b**, **`layer_backward`** uses **dH** to calculate **dW**, **dH_prev** and **db**. You have already studied the formulae in backpropogation. To calculate **dZ**, use the **`sigmoid_backward`** and **`relu_backward`** function. You might need to use _[np.dot()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html)_, _[np.sum()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html)_ for the rest. Remember to choose the axis correctly in db. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nj9YAXRZQso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graded\n",
        "\n",
        "def layer_backward(dH, memory, activation = 'relu'):\n",
        "    \n",
        "    # takes dH and the memory calculated in layer_forward and activation as input to calculate the dH_prev, dW, db\n",
        "    # performs the backprop depending upon the activation function\n",
        "    \n",
        "\n",
        "    linear_memory, activation_memory = memory\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dH,activation_memory) #write your code here\n",
        "        H_prev, W, b = linear_memory\n",
        "        m = H_prev.shape[1]\n",
        "        dW =   (1./m) * np.dot(dZ,H_prev.T)               #write your code here, use (1./m) and not (1/m)\n",
        "        db =   (1./m) * np.sum(dZ,axis=-1,keepdims=True) #write your code here, use (1./m) and not (1/m)\n",
        "        dH_prev = np.dot(W.T,dZ)                           #write your code here\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dH,activation_memory)#write your code here\n",
        "        H_prev, W, b = linear_memory\n",
        "        m = H_prev.shape[1]\n",
        "        dW = (1./m) * np.dot(dZ,H_prev.T)  #write your code here, use (1./m) and not (1/m)\n",
        "        db =  (1./m) * np.sum(dZ,axis=-1,keepdims=True) #write your code here, use (1./m) and not (1/m)\n",
        "        dH_prev = np.dot(W.T,dZ)   #write your code here\n",
        "    \n",
        "    return dH_prev, dW, db"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRUrqzU0ZQsx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "c59ddbc6-f26c-4243-b77c-4329728ff91a"
      },
      "source": [
        "# verify\n",
        "# l-1 has two neurons, l has three, m = 5\n",
        "# H_prev is (l-1, m)\n",
        "# W is (l, l-1)\n",
        "# b is (l, 1)\n",
        "# H should be (l, m)\n",
        "H_prev = np.array([[1,0, 5, 10, 2], [2, 5, 3, 10, 2]])\n",
        "W_sample = np.array([[10, 5], [2, 0], [1, 0]])\n",
        "b_sample = np.array([10, 5, 0]).reshape((3, 1))\n",
        "\n",
        "H, memory = layer_forward(H_prev, W_sample, b_sample, activation=\"relu\")\n",
        "np.random.seed(2)\n",
        "dH = np.random.rand(3,5)\n",
        "dH_prev, dW, db = layer_backward(dH, memory, activation = 'relu')\n",
        "print('dH_prev is \\n' , dH_prev)\n",
        "print('dW is \\n' ,dW)\n",
        "print('db is \\n', db)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dH_prev is \n",
            " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]\n",
            " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]\n",
            "dW is \n",
            " [[1.67565336 1.56891359]\n",
            " [1.39137819 1.4143854 ]\n",
            " [1.3597389  1.43013369]]\n",
            "db is \n",
            " [[0.37345476]\n",
            " [0.34414727]\n",
            " [0.29074635]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjkkxcqTZQs4",
        "colab_type": "text"
      },
      "source": [
        "You should get:<br>\n",
        "dH_prev is <br>\n",
        " [[5.6417525  0.66855959 6.86974666 5.46611139 4.92177244]<br>\n",
        " [2.17997451 0.12963116 2.74831239 2.17661196 2.10183901]]<br>\n",
        "dW is <br>\n",
        " [[1.67565336 1.56891359]<br>\n",
        " [1.39137819 1.4143854 ]<br>\n",
        " [1.3597389  1.43013369]]<br>\n",
        "db is <br>\n",
        " [[0.37345476]<br>\n",
        " [0.34414727]<br>\n",
        " [0.29074635]]<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2ZyYf3HZQs5",
        "colab_type": "text"
      },
      "source": [
        "### L_layer_backward\n",
        "\n",
        "**`L_layer_backward`** performs backpropagation for the whole network. Recall that the backpropagation for the last layer, i.e. the softmax layer, is different from the rest, hence it is outside the reversed `for` loop. You need to use the function **`layer_backward`** here in the loop with the activation function as **`relu`**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJQHTaKyZQs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graded\n",
        "\n",
        "def L_layer_backward(HL, Y, memories):\n",
        "    \n",
        "    # Takes the predicted value HL and the true target value Y and the \n",
        "    # memories calculated by L_layer_forward as input\n",
        "    \n",
        "    # returns the gradients calulated for all the layers as a dict\n",
        "\n",
        "    gradients = {}\n",
        "    L = len(memories) # the number of layers\n",
        "    m = HL.shape[1]\n",
        "    Y = Y.reshape(HL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Perform the backprop for the last layer that is the softmax layer\n",
        "    current_memory = memories[-1]\n",
        "    linear_memory, activation_memory = current_memory\n",
        "    dZ = HL - Y\n",
        "    H_prev, W, b = linear_memory\n",
        "    # Use the expressions you have used in 'layer_backward'\n",
        "    gradients[\"dH\" + str(L-1)] =  np.dot(W.T,dZ)                              #write your code here\n",
        "    gradients[\"dW\" + str(L)] =    (1./m) * np.dot(dZ,H_prev.T)                 #write your code here, use (1./m) and not (1/m)\n",
        "    gradients[\"db\" + str(L)] =    (1./m) * np.sum(dZ,axis=-1,keepdims=True)    #write your code here, use (1./m) and not (1/m)\n",
        "    \n",
        "    # Perform the backpropagation l-1 times\n",
        "    for l in reversed(range(L-1)):\n",
        "        # Lth layer gradients: \"gradients[\"dH\" + str(l + 1)] \", gradients[\"dW\" + str(l + 2)] , gradients[\"db\" + str(l + 2)]\n",
        "        current_memory = memories[l]\n",
        "        dH_prev_temp, dW_temp, db_temp = layer_backward(gradients[\"dH\" + str(l + 1)] ,current_memory,activation=\"relu\")\n",
        "         #write your code here\n",
        "        gradients[\"dH\" + str(l)] = dH_prev_temp #write your code here\n",
        "        gradients[\"dW\" + str(l + 1)] = dW_temp #write your code here\n",
        "        gradients[\"db\" + str(l + 1)] = db_temp #write your code here\n",
        "\n",
        "\n",
        "    return gradients"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZASIX-HxZQtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "a0892408-6fe6-4c51-fd4e-1776343b72dc"
      },
      "source": [
        "# verify\n",
        "# X is (784, 10)\n",
        "# parameters is a dict\n",
        "# HL should be (10, 10)\n",
        "x_sample = train_set_x[:, 10:20]\n",
        "y_sample = train_set_y[:, 10:20]\n",
        "\n",
        "HL, memories = L_layer_forward(x_sample, parameters=parameters)\n",
        "gradients  = L_layer_backward(HL, y_sample, memories)\n",
        "print('dW3 is \\n', gradients['dW3'])\n",
        "print('db3 is \\n', gradients['db3'])\n",
        "print('dW2 is \\n', gradients['dW2'])\n",
        "print('db2 is \\n', gradients['db2'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dW3 is \n",
            " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863\n",
            "   0.        ]\n",
            " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533\n",
            "   0.        ]\n",
            " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996\n",
            "   0.        ]\n",
            " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381\n",
            "   0.        ]\n",
            " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682\n",
            "   0.        ]\n",
            " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441\n",
            "   0.        ]\n",
            " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034\n",
            "   0.        ]\n",
            " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535\n",
            "   0.        ]\n",
            " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264\n",
            "   0.        ]\n",
            " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965\n",
            "   0.        ]]\n",
            "db3 is \n",
            " [[ 0.10031756]\n",
            " [ 0.00460183]\n",
            " [-0.00142942]\n",
            " [-0.0997827 ]\n",
            " [ 0.09872663]\n",
            " [ 0.00536378]\n",
            " [-0.10124784]\n",
            " [-0.00191121]\n",
            " [-0.00359044]\n",
            " [-0.00104818]]\n",
            "dW2 is \n",
            " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]\n",
            " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]\n",
            " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]\n",
            " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]\n",
            " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]\n",
            " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
            "db2 is \n",
            " [[ 0.013979  ]\n",
            " [-0.01329383]\n",
            " [ 0.01275707]\n",
            " [-0.01052957]\n",
            " [ 0.03179224]\n",
            " [-0.00039877]\n",
            " [ 0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK6V3DUyZQtH",
        "colab_type": "text"
      },
      "source": [
        "You should get:<br>\n",
        "\n",
        "dW3 is <br>\n",
        " [[ 0.02003701  0.0019043   0.01011729  0.0145757   0.00146444  0.00059863  0.        ]<br>\n",
        " [ 0.02154547  0.00203519  0.01085648  0.01567075  0.00156469  0.00060533   0.        ]<br>\n",
        " [-0.01718407 -0.00273711 -0.00499101 -0.00912135 -0.00207365  0.00059996   0.        ]<br>\n",
        " [-0.01141498 -0.00158622 -0.00607049 -0.00924709 -0.00119619  0.00060381   0.        ]<br>\n",
        " [ 0.01943173  0.0018421   0.00984543  0.01416368  0.00141676  0.00059682   0.        ]<br>\n",
        " [ 0.01045447  0.00063974  0.00637621  0.00863306  0.00050118  0.00060441   0.        ]<br>\n",
        " [-0.06338911 -0.00747251 -0.0242169  -0.03835708 -0.00581131  0.0006034   0.        ]<br>\n",
        " [ 0.01911373  0.001805    0.00703101  0.0120636   0.00138836 -0.00140535   0.        ]<br>\n",
        " [-0.01801603  0.0017357  -0.01489228 -0.02026076  0.00133528  0.00060264   0.        ]<br>\n",
        " [ 0.0194218   0.00183381  0.00594427  0.01187949  0.00141043 -0.00340965    0.        ]]<br>\n",
        "db3 is <br>\n",
        " [[ 0.10031756]<br>\n",
        " [ 0.00460183]<br>\n",
        " [-0.00142942]<br>\n",
        " [-0.0997827 ]<br>\n",
        " [ 0.09872663]<br>\n",
        " [ 0.00536378]<br>\n",
        " [-0.10124784]<br>\n",
        " [-0.00191121]<br>\n",
        " [-0.00359044]<br>\n",
        " [-0.00104818]]<br>\n",
        "dW2 is <br>\n",
        " [[ 4.94428956e-05  1.13215514e-02  5.44180380e-02]<br>\n",
        " [-4.81267081e-05 -2.96999448e-05 -1.81899582e-02]<br>\n",
        " [ 5.63424333e-05  4.77190073e-03  4.04810232e-02]<br>\n",
        " [ 1.49767478e-04 -1.89780927e-03 -7.91231369e-03]<br>\n",
        " [ 1.97866094e-04  1.22107085e-04  2.64140566e-02]<br>\n",
        " [ 0.00000000e+00 -3.75805770e-04  1.63906102e-05]<br>\n",
        " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]]<br>\n",
        "db2 is <br>\n",
        " [[ 0.013979  ]<br>\n",
        " [-0.01329383]<br>\n",
        " [ 0.01275707]<br>\n",
        " [-0.01052957]<br>\n",
        " [ 0.03179224]<br>\n",
        " [-0.00039877]<br>\n",
        " [ 0.        ]]<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaLipWIEZQtJ",
        "colab_type": "text"
      },
      "source": [
        "# Parameter Updates\n",
        "\n",
        "Now that we have calculated the gradients. let's do the last step which is updating the weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5pjvC3ZQtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graded\n",
        "\n",
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "\n",
        "    # parameters is the python dictionary containing the parameters W and b for all the layers\n",
        "    # gradients is the python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    # returns updated weights after applying the gradient descent update\n",
        "\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * gradients[\"dW\" + str(l+1)]) #write your code here\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * gradients[\"db\" + str(l+1)]) #write your code here\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e7_nIsQZQtQ",
        "colab_type": "text"
      },
      "source": [
        "Having defined the bits and pieces of the feedforward and the backpropagation, let's now combine all that to form a model. The list `dimensions` has the number of neurons in each layer specified in it. For a neural network with 1 hidden layer with 45 neurons, you would specify the dimensions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uJGCYTaZQtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dimensions = [784, 45, 10] #  three-layer model"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amgWJ6HYZQtZ",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "\n",
        "### L_layer_model\n",
        "\n",
        "This is a composite function which takes the training data as input **X**, ground truth label **Y**, the **dimensions** as stated above, **learning_rate**, the number of iterations **num_iterations** and if you want to print the loss, **print_loss**. You need to use the final functions we have written for feedforward, computing the loss, backpropagation and updating the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTYcNvToZQtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Graded\n",
        "\n",
        "def L_layer_model(X, Y, dimensions, learning_rate = 0.0075, num_iterations = 3000, print_loss=False):\n",
        "    \n",
        "    # X and Y are the input training datasets\n",
        "    # learning_rate, num_iterations are gradient descent optimization parameters\n",
        "    # returns updated parameters\n",
        "\n",
        "    np.random.seed(2)\n",
        "    losses = []                         # keep track of loss\n",
        "    \n",
        "    # Parameters initialization\n",
        "    parameters = initialize_parameters(dimensions)   #write your code here\n",
        " \n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation\n",
        "        HL, memories = L_layer_forward(X,parameters) #write your code here\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = compute_loss(HL,Y) #write your code here\n",
        "    \n",
        "        # Backward propagation\n",
        "        gradients = L_layer_backward(HL,Y,memories) #write your code here\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters =  update_parameters(parameters,gradients,learning_rate)  #write your code here\n",
        "                \n",
        "        # Printing the loss every 100 training example\n",
        "        if print_loss and i % 100 == 0:\n",
        "            print (\"Loss after iteration %i: %f\" %(i, loss))\n",
        "            losses.append(loss)\n",
        "            \n",
        "    # plotting the loss\n",
        "    plt.plot(np.squeeze(losses))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s6A8bvNZQth",
        "colab_type": "text"
      },
      "source": [
        "Since, it'll take a lot of time to train the model on 50,000 data points, we take a subset of 5,000 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rmb_3dL6ZQtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37bbd396-e7b7-4187-9e7c-3ae6da2fc049"
      },
      "source": [
        "train_set_x_new = train_set_x[:,0:5000]\n",
        "train_set_y_new = train_set_y[:,0:5000]\n",
        "train_set_x_new.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWg8i2z-ZQtp",
        "colab_type": "text"
      },
      "source": [
        "Now, let's call the function L_layer_model on the dataset we have created.This will take 10-20 mins to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXSxIgg_ZQtr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "de598dff-f7ad-4b8e-b626-21357181e955"
      },
      "source": [
        "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 2000, print_loss = True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 2.422624\n",
            "Loss after iteration 100: 2.129232\n",
            "Loss after iteration 200: 1.876095\n",
            "Loss after iteration 300: 1.604213\n",
            "Loss after iteration 400: 1.350205\n",
            "Loss after iteration 500: 1.144823\n",
            "Loss after iteration 600: 0.990554\n",
            "Loss after iteration 700: 0.876603\n",
            "Loss after iteration 800: 0.791154\n",
            "Loss after iteration 900: 0.725441\n",
            "Loss after iteration 1000: 0.673485\n",
            "Loss after iteration 1100: 0.631386\n",
            "Loss after iteration 1200: 0.596598\n",
            "Loss after iteration 1300: 0.567342\n",
            "Loss after iteration 1400: 0.542346\n",
            "Loss after iteration 1500: 0.520746\n",
            "Loss after iteration 1600: 0.501865\n",
            "Loss after iteration 1700: 0.485205\n",
            "Loss after iteration 1800: 0.470368\n",
            "Loss after iteration 1900: 0.457054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV5dn/8c83CwmEJBAS9rAJsotiiqBWsSqCtVrrUq1aa21Rq09rn25q+1RrW2u1y09braVqtdZaW7diRdFad0UMKPu+74QECAkQCFy/P2aCh3gSAsnJZLner9e8zpz7vmfmmsnJuc5s98jMcM4556pLijoA55xzTZMnCOecc3F5gnDOOReXJwjnnHNxeYJwzjkXlycI55xzcXmCcC2apE9LWhR1HM41R54gXMJIWinpjChjMLO3zGxglDFUkTRW0tpGWtbpkhZK2inpNUm9a2nbJ2yzM5zmjGr135a0UVKppIclpYXlvSSVVRtM0nfC+rGS9lervzKxa+4akicI16xJSo46BgAFmsT/k6Rc4Bng/4AcoBB4spZJngA+BDoBPwSekpQXzuss4CbgdKA30A/4CYCZrTaz9lUDMBzYDzwdM+/1sW3M7NEGXFWXYE3iA+1aF0lJkm6StExSsaR/SMqJqf9n+It1u6Q3JQ2NqXtE0h8kTZFUDpwW7ql8V9LscJonJaWH7Q/61V5b27D++5I2SFov6WvhL+L+NazH65J+LukdYCfQT9JVkhZI2iFpuaRrwrYZwItA95hf090PtS2O0BeAeWb2TzPbDdwGjJA0KM46HA2MBG41s11m9jQwB7ggbHIl8JCZzTOzrcBPga/UsNwvA2+a2cp6xu+aCE8QLgr/A3weOBXoDmwF7oupfxEYAHQGZgKPV5v+S8DPgUzg7bDsYmA80Bc4hpq/xGpsK2k88L/AGUB/YGwd1uUKYGIYyypgM3AOkAVcBfxW0kgzKwcmcPAv6vV12BYHhId0ttUyfClsOhSYVTVduOxlYXl1Q4HlZrYjpmxWTNuD5hWOd5HUqVpsIkgQ1fcQOkvaJGmFpN+GidI1EylRB+BapWuBG8xsLYCk24DVkq4ws0oze7iqYVi3VVK2mW0Pi/9lZu+E47uD7ybuDb9wkfQ8cGwty6+p7cXAn81sXsyyLzvEujxS1T70Qsz4G5JeBj5NkOjiqXVbxDY0s9VAh0PEA9AeKKpWtp0gicVruz1O2x411FeNZwLFMeUnA12Ap2LKFhJs24UEh6ceBX4DXFOHdXBNgO9BuCj0Bp6t+uULLAD2EfwyTZZ0Z3jIpRRYGU6TGzP9mjjz3BgzvpPgi60mNbXtXm3e8ZZT3UFtJE2QNE1SSbhuZ3Nw7NXVuC3qsOyalBHswcTKAnYcQdvq9VXj1ed1JfC0mZVVFZjZRjObb2b7zWwF8H0+PnTlmgFPEC4Ka4AJZtYhZkg3s3UEh4/OIzjMkw30CadRzPSJ6oJ4A9Az5n1+HaY5EEt4dc/TwK+ALmbWAZjCx7HHi7u2bXGQGq4aih2q9nbmASNipssAjgrLq5tHcO4kdu9iREzbg+YVjm8yswN7D5LaAhfxycNL1Rn+ndOs+B/LJVqqpPSYIQV4APi5wksvJeVJOi9snwlUEBy+aAfc0Yix/gO4StJgSe0IrgI6HG2ANILDO5WSJgDjYuo3AZ0kZceU1bYtDlL9qqE4Q9W5mmeBYZIuCE/A/xiYbWYL48xzMfARcGv49zmf4LxM1ZVIfwGuljREUgfgR8Aj1WZzPsG5k9diCyWdJqm3AvnAncC/atp4runxBOESbQqwK2a4DbgHmAy8LGkHMA04IWz/F4KTveuA+WFdozCzF4F7Cb7olsYsu6KO0+8AvkmQaLYS7A1NjqlfSHBJ6fLwkFJ3at8WR7oeRQSHcn4exnECcElVvaQHJD0QM8klQEHY9k7gwnAemNlLwF0E22Q1wd/m1mqLvBJ4zD75cJnjgHeB8vB1DsH2cc2E/IFBzsUnaTAwF0irfsLYudbA9yCciyHpfElpkjoCvwSe9+TgWitPEM4d7BqCexmWEVxNdF204TgXHT/E5JxzLq6E7UFIylfQAdh8SfMkfStOm7EKujv4KBx+HFM3XtIiSUsl3ZSoOJ1zzsWXyDupK4HvmNnM8BrrGZJeMbP51dq9ZWbnxBYo6IDtPuBMYC3wgaTJcaY9SG5urvXp06fh1sA551q4GTNmbDGzvHh1CUsQZraB4MYjzGyHpAUEt+/X+iUfGgUsNbPlAJL+TnDzVK3T9unTh8LCwnrF7ZxzrYmkVTXVNcpJakl9CK6Jfj9O9RhJsyS9qI977ezBwV0YrOXjvmGqz3uipEJJhUVF1bufcc45d6QSniAktSe4K/NGMyutVj0T6G1mI4DfAc8d7vzNbJKZFZhZQV5e3L0k55xzRyChCUJSKkFyeNzMnqleb2alVZ17mdkUgm4Zcgnuoo3tB6dnWOacc66RJPIqJgEPAQvM7Dc1tOkatkPSqDCeYuADYICkvpLaEHQFMDnePJxzziVGIq9iOongYSpzJH0Ult0C9AIwsweAC4HrJFUS9NNzSdifS6WkG4CpQDLwcLU+951zziVYi7pRrqCgwPwqJuecqztJM8ysIF6dd7XhnHMurlafICoq9zHpzWV8sLIk6lCcc65JafUJwgz+/M5Kfv7CAlrS4TbnnKuvVp8g0lOT+faZR/PRmm28OHfjoSdwzrlWotUnCIALRvZkYJdM7p66iL379kcdjnPONQmeIIDkJPGDCQNZsaWcv09fHXU4zjnXJHiCCJ02sDMn9M3hnleXUFbhDxBzzjlPECFJ3DRhEFvK9vCnN5dHHY5zzkXOE0SM43p15OzhXfnTW8vZvGN31OE451ykPEFU872zBrGncj/3vrok6lCccy5SniCq6ZubwaWjevHE9DUsLyqLOhznnIuMJ4g4vnn6ANJTkrh76qKoQ3HOuch4gogjLzONr5/SjxfnbmTm6q1Rh+Occ5HwBFGDr3+6H7nt07hzykLvgsM51yp5gqhBRloK3zpjANNXlvDqgs1Rh+Occ43OE0QtLvlUPv1yM/jlSwup9C44nHOtTCIfOZov6TVJ8yXNk/StOG0ukzRb0hxJ70oaEVO3Miz/SFIkTwFKTU7ie2cNZMnmMp6euTaKEJxzLjKJ3IOoBL5jZkOA0cD1koZUa7MCONXMhgM/BSZVqz/NzI6t6WlHjWH8sK4c16sDv3llMbv27IsqDOeca3QJSxBmtsHMZobjO4AFQI9qbd41s6rLhKYBPRMVz5GSxM0TBrOptIKH31kRdTjOOddoGuUchKQ+wHHA+7U0uxp4Mea9AS9LmiFpYuKiO7RRfXM4Y3BnHnh9GSXle6IMxTnnGk3CE4Sk9sDTwI1mVlpDm9MIEsQPYopPNrORwASCw1On1DDtREmFkgqLiooaOPqP/WD8IMr3VPL7/y5N2DKcc64pSWiCkJRKkBweN7NnamhzDPAgcJ6ZFVeVm9m68HUz8CwwKt70ZjbJzArMrCAvL6+hV+GAAV0yuej4fB6btpI1JTsTthznnGsqEnkVk4CHgAVm9psa2vQCngGuMLPFMeUZkjKrxoFxwNxExVpX3z7zaJKTxK9e9i44nHMtXyL3IE4CrgA+E16q+pGksyVdK+nasM2PgU7A/dUuZ+0CvC1pFjAdeMHMXkpgrHXSNTudr57Ul399tJ6567ZHHY5zziWUWlI3EgUFBVZYmNhbJkp37+XUu15jaPds/vq1ExK6LOecSzRJM2q6lcDvpD5MWemp3PCZAby9dAtvLk7cSXHnnIuaJ4gjcPnoXvTs2JY7X1zI/v0tZw/MOedieYI4AmkpyXzvrIHM31DKv2atizoc55xLCE8QR+hzx3RnWI8sfjV1Mbv3ehcczrmWxxPEEUpKEjeNH8y6bbv467RVUYfjnHMNzhNEPZw8IJdPD8jl968tZfuuvVGH45xzDcoTRD3dNGEQ23ft5XevLok6FOeca1CeIOppaPdsvliQz5/fXcm89X7znHOu5fAE0QBunjCYju1SufmZOezzy16dcy2EJ4gGkN0ulf87Zwiz127nL++tjDoc55xrEJ4gGsi5I7pzytF5/GrqItZv2xV1OM45V2+eIBqIJH7++WHsM+PWyfOiDsc55+rNE0QDys9px41nHM0r8zfx0tyNUYfjnHP14gmigV19cl8Gdc3ktsnz2LHb741wzjVfniAaWGpyEndecAybduzmV1P9wULOuebLE0QCHJvfgSvH9OEv01Yxc/XWqMNxzrkj4gkiQb4z7mi6ZKZzyzNz2Ltvf9ThOOfcYUvkM6nzJb0mab6keZK+FaeNJN0raamk2ZJGxtRdKWlJOFyZqDgTJTM9lZ+cN5SFG3fw4Fsrog7HOecOWyL3ICqB75jZEGA0cL2kIdXaTAAGhMNE4A8AknKAW4ETgFHArZI6JjDWhDhraFfGDenCPa8uZnXxzqjDcc65w5KwBGFmG8xsZji+A1gA9KjW7DzgLxaYBnSQ1A04C3jFzErMbCvwCjA+UbEm0k/OG0pKUhI/fG4OLen53865lq9RzkFI6gMcB7xfraoHsCbm/dqwrKbyePOeKKlQUmFRUdN7RnS37LZ8d9zRvLVkC5NnrY86HOecq7OEJwhJ7YGngRvNrLSh529mk8yswMwK8vLyGnr2DeKKMX0Ykd+B25+fz7ade6IOxznn6iShCUJSKkFyeNzMnonTZB2QH/O+Z1hWU3mzlJwkfnH+cLbt2ssvpiyMOhznnKuTRF7FJOAhYIGZ/aaGZpOBL4dXM40GtpvZBmAqME5Sx/Dk9LiwrNka0j2Lr53clycL1/D+8uKow3HOuUNK5B7EScAVwGckfRQOZ0u6VtK1YZspwHJgKfAn4BsAZlYC/BT4IBxuD8uatW+dMYD8nLbc/OwcKir3RR2Oc87VSi3pypqCggIrLCyMOoxavbG4iCsfns6NZwzgxjOOjjoc51wrJ2mGmRXEq/M7qRvZqUfnce6I7tz/2jKWbi6LOhznnKuRJ4gI/N85Q0hPTeKWZ+ew3x9R6pxrojxBRCAvM41bzh7M9BUl/HPGmkNP4JxzEfAEEZGLC/IZ1SeHO6YsZEtZRdThOOfcJ3iCiEhSkrjjC8PYuaeSn/57ftThOOfcJ3iCiFD/zplcN7Y///poPW8sbnrdhDjnWjdPEBH7xtij6JebwW2T57Gn0p8b4ZxrOjxBRCw9NZkff24IK7aU8+d3/LkRzrmmwxNEEzB2YGfOGNyZe19dwubS3VGH45xzgCeIJuNHnx3C3n3GnS95Z37OuabBE0QT0Sc3g699ui/PzFzHjFVbow7HOec8QTQl15/Wny5Zadw2eZ7fYe2ci5wniCYkIy2FW84ezJx12/0Oa+dc5DxBNDHnjuhOQe+O3PXSIrbv2ht1OM65VswTRBMjidvOHUrJzj3c858lUYfjnGvFPEE0QcN6ZHPpqF48+t5KlmzaEXU4zrlWKpGPHH1Y0mZJc2uo/17Mk+bmStonKSesWylpTljXtJ8AlCDfHTeQjDbJ3Pb8PFrSQ52cc81HIvcgHgHG11RpZneb2bFmdixwM/BGtceKnhbWx33SUUuXk9GG74wbyDtLi5k6b1PU4TjnWqGEJQgzexOo63OkLwWeSFQszdVlJ/RiYJdMfvbCfHbv9WdYO+caV+TnICS1I9jTeDqm2ICXJc2QNPEQ00+UVCipsKioZfWImpKcxK3nDmHt1l1MenN51OE451qZyBME8DngnWqHl042s5HABOB6SafUNLGZTTKzAjMryMvLS3Ssje7Eo3L57PBu3P/6UtZt2xV1OM65VqQpJIhLqHZ4yczWha+bgWeBURHE1WTcfPYgAO6YsiDiSJxzrUmkCUJSNnAq8K+YsgxJmVXjwDgg7pVQrUXPju247tT+vDB7A+8tK446HOdcK5HIy1yfAN4DBkpaK+lqSddKujam2fnAy2ZWHlPWBXhb0ixgOvCCmb2UqDibi2tO7UePDm35yfPzqNznDxZyziVeSqJmbGaX1qHNIwSXw8aWLQdGJCaq5is9NZn/O2cw1/51Jn+bvpovj+kTdUjOuRauKZyDcHV01tCunNS/E79+eTEl5XuiDsc518J5gmhGJHHr54ZSVlHJr19eFHU4zrkWzhNEM3N0l0y+PKY3f5u+mrnrtkcdjnOuBfME0QzdeMbRdGzXhp94P03OuQTyBNEMZbdN5ftnDeSDlVuZPGt91OE451ooTxDN1EUF+Qzvkc0vpiykvKIy6nCccy2QJ4hmKjlJ3HbuEDaW7ub+15dGHY5zrgXyBNGMHd87hy8c14M/vbmCVcXlh57AOecOgyeIZu4HEwaRmixum+wnrJ1zDcsTRDPXJSud/x03kNcWFTFlzsaow3HOtSCeIFqAK8f0ZliPLG57fh6lu/dGHY5zroXwBNECpCQn8Yvzj6G4rIK7XloYdTjOuRbCE0QLMbxnNl85sS+Pv7+aGau2Rh2Oc64F8ATRgnxn3NF0y0rnlmfmsNe7BHfO1ZMniBYkIy2Fn5w3jEWbdvDgWyuiDsc518x5gmhhzhzShfFDu3LPq4tZXbwz6nCcc81YIp8o97CkzZLiPi5U0lhJ2yV9FA4/jqkbL2mRpKWSbkpUjC3VbecOJSUpiR8+N8fvjXDOHbFE7kE8Aow/RJu3zOzYcLgdQFIycB8wARgCXCppSALjbHG6ZqfzvbMG8taSLd6Zn3PuiCUsQZjZm0DJEUw6ClhqZsvNbA/wd+C8Bg2uFbh8dG9G5Hfgp/+ez7ad/vQ559zhq1OCkPQtSVkKPCRppqRxDbD8MZJmSXpR0tCwrAewJqbN2rCsptgmSiqUVFhUVNQAIbUMyUnijvOHsXXnXn7p90Y4545AXfcgvmpmpcA4oCNwBXBnPZc9E+htZiOA3wHPHclMzGySmRWYWUFeXl49Q2pZhnbP5uqT+/LE9DVMX3EkO3POudasrglC4evZwGNmNi+m7IiYWamZlYXjU4BUSbnAOiA/pmnPsMwdgRvPGECPDm255dk57Kn0eyOcc3VX1wQxQ9LLBAliqqRMoF7fNpK6SlI4PiqMpRj4ABggqa+kNsAlwOT6LKs1a9cmhZ99fhhLN5fxxzeWRR2Oc64ZSalju6uBY4HlZrZTUg5wVW0TSHoCGAvkSloL3AqkApjZA8CFwHWSKoFdwCUWXJNZKekGYCqQDDwc7rG4I3TaoM589phu/O61pZwzojt9czOiDsk51wyoLtfJSzoJ+MjMyiVdDowE7jGzVYkO8HAUFBRYYWFh1GE0SZtLd3P6b95geI9sHv/aCYQ7b865Vk7SDDMriFdX10NMfwB2ShoBfAdYBvylgeJzjaBzVjo/GD+Id5cV8+yHfkrHOXdodU0QleHhn/OA35vZfUBm4sJyifClUb0Y2asDP3thASXlfm+Ec652dU0QOyTdTHB56wuSkgjPJ7jmIylJ3PGF4ZTu2ssvpiyIOhznXBNX1wTxRaCC4H6IjQSXnt6dsKhcwgzqmsXXT+nHP2es5b1lxVGH45xrwuqUIMKk8DiQLekcYLeZ+TmIZuqbnxlAr5x2/PDZOVRU7os6HOdcE1XXrjYuBqYDFwEXA+9LujCRgbnEadsmmZ99fhjLt5Rz/2t+b4RzLr663gfxQ+BTZrYZQFIe8B/gqUQF5hLrlKPzOO/Y7vzh9WV8bkR3+nduH3VIzrkmpq7nIJKqkkOo+DCmdU3Ujz47hPTUJH74rD83wjn3SXX9kn9J0lRJX5H0FeAFYEriwnKNIS8zjVvOHsz7K0p48oM1h57AOdeq1PUk9feAScAx4TDJzH6QyMBc47i4IJ8x/Tpx2/PzWLixNOpwnHNNSJ0PE5nZ02b2v+HwbCKDco0nKUncc+mxZKWnct1fZ1K6e2/UITnnmohaE4SkHZJK4ww7JPnPzRaic2Y69102ktUlO/neP2f5+QjnHHCIBGFmmWaWFWfINLOsxgrSJd6n+uRw84RBTJ23iUlvLo86HOdcE+BXIrkDrj65L58d3o1fvrTQ77J2znmCcB+TxC8vPIa+uRn8zxMz2bh9d9QhOeci5AnCHaR9WgoPXH48O/fs4/q/zWTvPn9MqXOtVcIShKSHJW2WNLeG+sskzZY0R9K74bMmqupWhuUfSfInADWyAV0y+eUFxzBj1Vbu8F5fnWu1ErkH8Qgwvpb6FcCpZjYc+CnBfRaxTjOzY2t60pFLrM+N6M5VJ/Xhz++s5PlZ66MOxzkXgYQlCDN7Eyippf5dM9savp1G0IW4a0JuOXswBb078oOnZ7Nk046ow3HONbKmcg7iauDFmPcGvCxphqSJtU0oaaKkQkmFRUVFCQ2ytUlNTuL3XxpJuzbJXPvXGZRVVEYdknOuEUWeICSdRpAgYrvuONnMRgITgOslnVLT9GY2ycwKzKwgLy8vwdG2Pl2z0/ndpSNZsaWcHzw122+ic64ViTRBSDoGeBA4z8wOXHhvZuvC183As8CoaCJ0AGOO6sT3xw/ihTkbeOjtFVGH45xrJJElCEm9gGeAK8xscUx5hqTMqnFgHBD3SijXeK45pR/jhnThFy8uZPqKGk8tOedakERe5voE8B4wUNJaSVdLulbStWGTHwOdgPurXc7aBXhb0iyCp9i9YGYvJSpOVzeS+NXFI+iV044b/jaTzTv8JjrnWjq1pGPKBQUFVljot00k0sKNpXz+vnc4pmcH/va1E0hJjvw0lnOuHiTNqOl2Av/vdodlUNcs7vzCMUxfUcJdUxdFHY5zLoE8QbjD9vnjenDF6N5MenM5L83dEHU4zrkE8QThjsiPzhnMsfkd+O4/Z7OsqCzqcJxzCeAJwh2RtJRk7r9sJG1SkrjurzPYucdvonOupfEE4Y5Y9w5tufeS41iyuYxvPvEhFZX7og7JOdeAPEG4ejl5QC63nzuU/yzYzDWPzWD3Xk8SzrUUniBcvV0xpg+/+MJw3lhcxNWPfuCHm5xrITxBuAZx6ahe/OrCEby3rJivPPwBO3bvjTok51w9eYJwDeaC43tyzyXHMWP1Vq54aDrbd3mScK458wThGtTnRnTn/stGMm/9di57cBpby/dEHZJz7gh5gnAN7qyhXZl0RQGLN5Vx6Z+msaWsIuqQnHNHwBOES4jTBnXm4Ss/xcricr74x/fYVOqd+znX3HiCcAlz8oBcHr1qFBu37+biP77Hum27og7JOXcYPEG4hDqhXyce+9oJlJTv4Yt/fI81JTujDsk5V0eeIFzCjezVkb99bTRlFZVc9MB7LPe+m5xrFjxBuEYxvGc2T3x9NHv37efiP05j8aYdUYfknDuEhCYISQ9L2iwp7iNDFbhX0lJJsyWNjKm7UtKScLgykXG6xjG4WxZPXjOaJMElk6Yxf31p1CE552qR6D2IR4DxtdRPAAaEw0TgDwCScoBbgROAUcCtkjomNFLXKPp3zuQf14whPSWJS/80jVlrtkUdknOuBglNEGb2JlDbE+7PA/5igWlAB0ndgLOAV8ysxMy2Aq9Qe6JxzUif3AyevGYMWW1TuPzB95mxqraPiHMuKlGfg+gBrIl5vzYsq6n8EyRNlFQoqbCoqChhgbqGlZ/Tjn9cM4bczDSueGg6by/ZEnVIzrlqok4Q9WZmk8yswMwK8vLyog7HHYZu2W15cuJoenZsyxUPv8/dUxeyd9/+qMNyzoWiThDrgPyY9z3DsprKXQvTOSudZ79xEhcd35P7XlvGhQ+8x8ot5VGH5Zwj+gQxGfhyeDXTaGC7mW0ApgLjJHUMT06PC8tcC5SRlsJdF47g/stGsqKojM/e+xb/LFyDmUUdmnOtWkoiZy7pCWAskCtpLcGVSakAZvYAMAU4G1gK7ASuCutKJP0U+CCc1e1m5mcyW7izh3fj2PwOfPvJj/jeU7N5fXERd3x+ONntUqMOzblWSS3pV1pBQYEVFhZGHYarp337jQfeWMZvX1lM58w0fvvFYzmhX6eow3KuRZI0w8wK4tVFfYjJuU9IThLXn9afp687kTbh/RK/fnmRn8B2rpF5gnBN1oj8DrzwzU9zwcie/O6/S7nogfdYVewnsJ1rLJ4gXJOWkZbC3ReN4L4vjWR5URln3/MWT89Y6yewnWsEniBcs/DZY7rx4o2nMLRHNt/55yy++feP/JnXziWYJwjXbPTo0JYnvj6a7501kClzNnD2PW/xwUq/uM25RPEE4ZqV2BPYKcnii398j9+8vIhKP4HtXIPzBOGapWPDE9hfGNmTe/+7lHN+9zavLdrs5yaca0CeIFyz1T4thV9dNIIHLh/Jrr37uOrPH3DJpGl8uHpr1KE51yJ4gnDN3vhh3Xjl26dy+3lDWVZUxvn3v8u1j81g6WZ/tKlz9eF3UrsWpbyikofeXsEf31jG7sr9XHR8T24842i6ZqdHHZpzTVJtd1J7gnAtUnFZBb9/bSl/nbaKJImrTurLdace5f06OVeNJwjXaq0p2clvXlnMcx+tIys9lW+MPYorT+xDempy1KE51yR4gnCt3vz1pdw1dSGvLyqiW3Y63z7jaL4wsgcpyX4azrVu3lmfa/WGdM/ikatG8feJo+mSlc73n57N+HveYuq8jX5prHM18AThWpXR/Trx7DdO5IHLj2e/Gdc8NoML/vAuby4uYv9+TxTOxfJDTK7Vqty3n6dmrOW3/1nMptIK+uZmcNkJvbjo+Hw/me1ajcjOQUgaD9wDJAMPmtmd1ep/C5wWvm0HdDazDmHdPmBOWLfazM491PI8QbgjsXvvPl6au5HHpq1ixqqtpKUkce6I7lw+ujcj8jtEHZ5zCRVJgpCUDCwGzgTWEjw+9FIzm19D+/8BjjOzr4bvy8ys/eEs0xOEq6/560v56/ureO7Ddezcs49jemZz+ejefO6Y7rRt41c+uZYnqgQxBrjNzM4K398MYGa/qKH9u8CtZvZK+N4ThItM6e69PPfhOh57bxVLNpeRlZ7CRQX5XHZCL/rlHdbH0rkmrbYEkZLA5fYA1sS8XwucEK+hpN5AX+C/McXpkgqBSuBOM3uuhmknAhMBevXq1QBhOwdZ6al8eUwfrhjdm+krSnhs2ioefXclD729gpP753L56N6cMXTcteUAABGNSURBVLizXybrWrREJojDcQnwlJntiynrbWbrJPUD/itpjpktqz6hmU0CJkGwB9E44brWQhIn9OvECf06sXnHbv7xwRr+9v5qrv3rDLpmpXPpqF5cOiqfzlnelYdreRKZINYB+THve4Zl8VwCXB9bYGbrwtflkl4HjgM+kSCcayydM9O54TMDuPbUo3htURGPTVvFb/+zmN/9dwljB3bm7OFdOX1wF7Lb+hVQrmVIZIL4ABggqS9BYrgE+FL1RpIGAR2B92LKOgI7zaxCUi5wEnBXAmN1rs5SkpM4c0gXzhzShZVbyvnb9NU8P2s9/1mwidRkcVL/XCYM68qZQ7qSk9Em6nCdO2KJvsz1bOD/EVzm+rCZ/VzS7UChmU0O29wGpJvZTTHTnQj8EdhPcDPf/zOzhw61PD9J7aKyf78xa+02Xpq7kSlzN7CmZBfJSWJ0vxwmDOvGuKFd6Jzph6Fc0+N9MTnXiMyMeetLeXHuBl6cs5HlW8qR4FN9cpgwrCvjh3WlW3bbqMN0DvAE4VxkzIzFm8oOJItFm3YAcFyvDkwY1pUJw7qRn9Mu4ihda+YJwrkmYllRWXAYas4G5q0vBWBYjyzOHNyVkwd04pieHUj1S2ddI/IE4VwTtLp4Jy/N28CUORuZtXYbZpDRJpnR/TpxUv9cTh6Qy4DO7ZEUdaiuBfME4VwTt7V8D+8tL+adpVt4Z+kWVhbvBCAvM42TjgoSxkn9c+newc9duIblCcK5Zmbt1p28u7SYt5du4d1lW9hStgeAfrkZYbLoxJh+ud7rrKs3TxDONWNmxqJNO3h7yRbeXVbMtOXF7NyzjyTB8B7ZnNg/l9H9OnFszw6eMNxh8wThXAuyp3I/s9ZuO3A46sPV26gMH3Z0VF4Gx/XqyHG9OnBcfkeO7tLe+4tytfIE4VwLVl5Ryaw12/hwzTY+XL2VD1dvo7g8OCTVrk0yx/TMDpJGfgeO7dXBb9hzB4mqN1fnXCPISEvhxP65nNg/FwgOSa0p2cWHa4Jk8eGabTz41nL27gt+DPbo0DbYwwj3NIZ2zyItxZ914T7JE4RzLYwkenVqR69O7Tjv2B5A8NS8eetLgz2MNdv4cPU2/j17AwBtkpMY2DWTwd0yGdwt68DgnQ46TxDOtQLpqckc37sjx/fueKBsU+nucA9jK/PWlfLqgs38o3DtgfoeHdoyuFsWQ2ISR6+cdiQl+X0ZrYUnCOdaqS5Z6YwP+4aC4NBU0Y4K5m8oZcGGHSzYUMqCDaW8tmgz+8KT4BltksO9jY/3NAZ1zSQjzb9KWiI/Se2cq9XuvftYsqmMBRtKw+QRDKW7KwGQIL9jO47Ky+CovPYc1bk9/Tu356i89t7deTPgJ6mdc0csPTWZ4T2zGd4z+0CZmbF++24WrA+SxeLNZSzbXMZ7y4vZvXf/gXYd26UGSSMvTBqdgyTSs2M7kv1QVZPnexDOuQazf7+xbtsulhWVsayonGVFZSzdXMbyorIDd4MDtElJom+nDI7qnEH/vPb0zcugV047euVkkNu+jfc/1Yh8D8I51yiSkkR+Tjvyc9oxduDBddt27gmSxuayMIGUsWDDDl6au5H9Mb9T27VJDpNFO3p3Cl57dcqgd047enRs673dNqKEJghJ44F7CJ4o96CZ3Vmt/ivA3Xz8rOrfm9mDYd2VwI/C8p+Z2aOJjNU5l1gd2rXh+N5tDrqSCqCich9rSnaxuqSc1cU7WVWyk9XFO1mxpZw3FhdRUfnxIaskQfcObT9OHDkZ9O7UjvyOQfLo2C7V9z4aUMIShKRk4D7gTGAt8IGkyWY2v1rTJ83shmrT5gC3AgWAATPCabcmKl7nXDTSUpLpH57Yrm7/fmPzjgpWl+xkVXE5q0t2huM7mTpvEyXlew5qn56aRPcObekRDt2rvXbNTqdNiu+B1FUi9yBGAUvNbDmApL8D5wHVE0Q8ZwGvmFlJOO0rwHjgiQTF6pxrgpKSRNfsdLpmpzOqb84n6nfs3svqkp2s3bqLdVt3sX7bLtZtC14XbNjBlrKKg9pL0Dkz7aCk0aNjW7plt6VLVhpds9Lp1D7NT6CHEpkgegBrYt6vBU6I0+4CSacAi4Fvm9maGqbtkahAnXPNU2Z6KkO7ZzO0e3bc+t1797Fh++4gcWwNkkdVApm7bjsvz9vEnn37D5omOUl0zkyjS1b6gaTRJTudrlnB0DkrSFjtW8G9H1Gv4fPAE2ZWIeka4FHgM4czA0kTgYkAvXr1avgInXPNVnpqMn1zM+ibmxG3fv9+Y0t5BRu27WZj6W42lwavG7dXsKl0N8uKynl3WTE7wns+YrVPSwkSSHY6XTLTyctMIy8zjdz2aQfG89qn0aEZnxdJZIJYB+THvO/JxyejATCz4pi3DwJ3xUw7ttq0r8dbiJlNAiZBcJlrfQJ2zrUuSUmic2Y6nTPTGVFLu/KKSjaVViWRijCJ7GZTaTC8v6KEorIK9lTu/8S0qck6kDRy2wdJ40ACiUkqORltyEpPaVLJJJEJ4gNggKS+BF/4lwBfim0gqZuZbQjfngssCMenAndIqrrcYRxwcwJjdc65GmWkpdAvrz398j55Ir2KmbGjopKiHRUHDVvKwvGyYK9k7rrtFJfvOdB9SazUZJGT0YacjDQ6ZbShU/s25GS0CceDJJLbPqhvjISSsARhZpWSbiD4sk8GHjazeZJuBwrNbDLwTUnnApVACfCVcNoSST8lSDIAt1edsHbOuaZIElnpqWSlB3eP12b/fmPrzj0UlX2cRIrL9lBcvoeSsj0Ul1dQXL6HNWt2Uly2h7KKTx7igo8TSu+cDP5x7ZiGXye/k9o555q23Xv3UVK+h5LyPWwpq4gZ30NJeQVJEndecMwRzdvvpHbOuWYsPTWZ7uGluY3J7xhxzjkXlycI55xzcXmCcM45F5cnCOecc3F5gnDOOReXJwjnnHNxeYJwzjkXlycI55xzcbWoO6klFQGrjnDyXGBLA4bT0Dy++vH46sfjq5+mHF9vM8uLV9GiEkR9SCqs6XbzpsDjqx+Pr348vvpp6vHVxA8xOeeci8sThHPOubg8QXxsUtQBHILHVz8eX/14fPXT1OOLy89BOOeci8v3IJxzzsXlCcI551xcrS5BSBovaZGkpZJuilOfJunJsP59SX0aMbZ8Sa9Jmi9pnqRvxWkzVtJ2SR+Fw48bK75w+SslzQmX/YnH9ylwb7j9Zksa2YixDYzZLh9JKpV0Y7U2jbr9JD0sabOkuTFlOZJekbQkfO1Yw7RXhm2WSLqyEeO7W9LC8O/3rKQONUxb62chgfHdJmldzN/w7BqmrfV/PYHxPRkT20pJH9UwbcK3X72ZWasZCJ6NvQzoB7QBZgFDqrX5BvBAOH4J8GQjxtcNGBmOZwKL48Q3Fvh3hNtwJZBbS/3ZwIuAgNHA+xH+rTcS3AQU2fYDTgFGAnNjyu4CbgrHbwJ+GWe6HGB5+NoxHO/YSPGNA1LC8V/Gi68un4UExncb8N06/P1r/V9PVHzV6n8N/Diq7VffobXtQYwClprZcjPbA/wdOK9am/OAR8Pxp4DTJakxgjOzDWY2MxzfASwAejTGshvQecBfLDAN6CCpWwRxnA4sM7MjvbO+QZjZm0BJteLYz9ijwOfjTHoW8IqZlZjZVuAVYHxjxGdmL5tZZfh2GtCzoZdbVzVsv7qoy/96vdUWX/i9cTHwREMvt7G0tgTRA1gT834tn/wCPtAm/CfZDnRqlOhihIe2jgPej1M9RtIsSS9KGtqogYEBL0uaIWlinPq6bOPGcAk1/2NGuf0AupjZhnB8I9AlTpumsh2/SrBHGM+hPguJdEN4COzhGg7RNYXt92lgk5ktqaE+yu1XJ60tQTQLktoDTwM3mllpteqZBIdNRgC/A55r5PBONrORwATgekmnNPLyD0lSG+Bc4J9xqqPefgex4FhDk7zWXNIPgUrg8RqaRPVZ+ANwFHAssIHgME5TdCm17z00+f+l1pYg1gH5Me97hmVx20hKAbKB4kaJLlhmKkFyeNzMnqleb2alZlYWjk8BUiXlNlZ8ZrYufN0MPEuwKx+rLts40SYAM81sU/WKqLdfaFPVYbfwdXOcNpFuR0lfAc4BLguT2CfU4bOQEGa2ycz2mdl+4E81LDfq7ZcCfAF4sqY2UW2/w9HaEsQHwABJfcNfmZcAk6u1mQxUXTFyIfDfmv5BGlp4zPIhYIGZ/aaGNl2rzolIGkXwN2yUBCYpQ1Jm1TjBycy51ZpNBr4cXs00GtgeczilsdT4yy3K7Rcj9jN2JfCvOG2mAuMkdQwPoYwLyxJO0njg+8C5ZrazhjZ1+SwkKr7Yc1rn17DcuvyvJ9IZwEIzWxuvMsrtd1iiPkve2APBVTaLCa5w+GFYdjvBPwNAOsGhiaXAdKBfI8Z2MsHhhtnAR+FwNnAtcG3Y5gZgHsFVGdOAExsxvn7hcmeFMVRtv9j4BNwXbt85QEEj/30zCL7ws2PKItt+BIlqA7CX4Dj41QTntF4FlgD/AXLCtgXAgzHTfjX8HC4FrmrE+JYSHL+v+gxWXdXXHZhS22ehkeJ7LPxszSb40u9WPb7w/Sf+1xsjvrD8karPXEzbRt9+9R28qw3nnHNxtbZDTM455+rIE4Rzzrm4PEE455yLyxOEc865uDxBOOeci8sThGvyJL0bvvaR9KUGnvct8ZaVKJI+n6geZKuvSwPNc7ikRxp6vq558MtcXbMhaSxBL57nHMY0KfZxx3Px6svMrH1DxFfHeN4luOdmSz3n84n1StS6SPoP8FUzW93Q83ZNm+9BuCZPUlk4eifw6bD//G9LSg6fXfBB2HHbNWH7sZLekjQZmB+WPRd2ijavqmM0SXcCbcP5PR67rPBO8LslzQ377P9izLxfl/SUgmcmPB5zZ/adCp7lMVvSr+Ksx9FARVVykPSIpAckFUpaLOmcsLzO6xUz73jrcrmk6WHZHyUlV62jpJ8r6LBwmqQuYflF4frOkvRmzOyfJ7gT2bU2Ud+p54MPhxqAsvB1LDHPcgAmAj8Kx9OAQqBv2K4c6BvTtupu5bYEXRp0ip13nGVdQNDFdjJBb6urCZ7XMZagh9+eBD+w3iO4A74TsIiP98o7xFmPq4Bfx7x/BHgpnM8Agjtx0w9nveLFHo4PJvhiTw3f3w98ORw34HPh+F0xy5oD9KgeP3AS8HzUnwMfGn9IqWsica4JGgccI+nC8H02wRftHmC6ma2IaftNSeeH4/lhu9r6YDoZeMLM9hF0rvcG8CmgNJz3WgAFTwvrQ9Btx27gIUn/Bv4dZ57dgKJqZf+woNO5JZKWA4MOc71qcjpwPPBBuIPTlo87BdwTE98M4Mxw/B3gEUn/AGI7itxM0E2Ea2U8QbjmTMD/mNlBndiF5yrKq70/AxhjZjslvU7wS/1IVcSM7yN4+lpl2Pnf6QSdPN4AfKbadLsIvuxjVT8JaNRxvQ5BwKNmdnOcur1mVrXcfYTfA2Z2raQTgM8CMyQdb2bFBNtqVx2X61oQPwfhmpMdBI9irTIVuE5BF+lIOjrsGbO6bGBrmBwGETwKtcrequmreQv4Yng+II/g0ZLTawpMwTM8si3oQvzbwIg4zRYA/auVXSQpSdJRBB24LTqM9aoudl1eBS6U1DmcR46k3rVNLOkoM3vfzH5MsKdT1V320TTFnkZdwvkehGtOZgP7JM0iOH5/D8HhnZnhieIi4j++8yXgWkkLCL6Ap8XUTQJmS5ppZpfFlD8LjCHobdOA75vZxjDBxJMJ/EtSOsGv9/+N0+ZN4NeSFPMLfjVB4ski6P1zt6QH67he1R20LpJ+RPDEsiSC3kavB2p7BOvdkgaE8b8arjvAacALdVi+a2H8MlfnGpGkewhO+P4nvL/g32b2VMRh1UhSGvAGwdPParxc2LVMfojJucZ1B9Au6iAOQy/gJk8OrZPvQTjnnIvL9yCcc87F5QnCOedcXJ4gnHPOxeUJwjnnXFyeIJxzzsX1/wEBuOsThV5Z+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM5mfAHxZQty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, parameters):\n",
        "    \n",
        "    # Performs forward propogation using the trained parameters and calculates the accuracy\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_layer_forward(X, parameters)\n",
        "    \n",
        "    p = np.argmax(probas, axis = 0)\n",
        "    act = np.argmax(y, axis = 0)\n",
        "\n",
        "    print(\"Accuracy: \"  + str(np.sum((p == act)/m)))\n",
        "        \n",
        "    return p"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3nByfKWZQt3",
        "colab_type": "text"
      },
      "source": [
        "Let's see the accuray we get on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COU954xrZQt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a18b55e-af2d-40d5-9add-9d142510bf0f"
      },
      "source": [
        "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8774000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqgz9bbLZQuB",
        "colab_type": "text"
      },
      "source": [
        "We get ~ 88% accuracy on the training data. Let's see the accuray on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbutrCayZQuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6d39832-ea6e-43a7-ada3-4f416ef6adf0"
      },
      "source": [
        "pred_test = predict(test_set_x, test_set_y, parameters)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8674000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykKwqmbtZQuL",
        "colab_type": "text"
      },
      "source": [
        "It is ~87%. You can train the model even longer and get better result. You can also try to change the network structure. \n",
        "<br>Below, you can see which all numbers are incorrectly identified by the neural network by changing the index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzjJHl4yZQuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "f14b140a-c170-4701-a29e-827f71614a1c"
      },
      "source": [
        "index  = 3474\n",
        "k = test_set_x[:,index]\n",
        "k = k.reshape((28, 28))\n",
        "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
        "plt.imshow(k, cmap='gray')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fabd7f617b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wsJ9tvnkkyQ",
        "colab_type": "text"
      },
      "source": [
        "## **Training for 5000 iterations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Nw162Cmku9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2384aec-8960-4942-f99f-ba79c136e0d6"
      },
      "source": [
        "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 5000, print_loss = True)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 2.422624\n",
            "Loss after iteration 100: 2.129232\n",
            "Loss after iteration 200: 1.876095\n",
            "Loss after iteration 300: 1.604213\n",
            "Loss after iteration 400: 1.350205\n",
            "Loss after iteration 500: 1.144823\n",
            "Loss after iteration 600: 0.990554\n",
            "Loss after iteration 700: 0.876603\n",
            "Loss after iteration 800: 0.791154\n",
            "Loss after iteration 900: 0.725441\n",
            "Loss after iteration 1000: 0.673485\n",
            "Loss after iteration 1100: 0.631386\n",
            "Loss after iteration 1200: 0.596598\n",
            "Loss after iteration 1300: 0.567342\n",
            "Loss after iteration 1400: 0.542346\n",
            "Loss after iteration 1500: 0.520746\n",
            "Loss after iteration 1600: 0.501865\n",
            "Loss after iteration 1700: 0.485205\n",
            "Loss after iteration 1800: 0.470368\n",
            "Loss after iteration 1900: 0.457054\n",
            "Loss after iteration 2000: 0.445034\n",
            "Loss after iteration 2100: 0.434120\n",
            "Loss after iteration 2200: 0.424141\n",
            "Loss after iteration 2300: 0.414967\n",
            "Loss after iteration 2400: 0.406498\n",
            "Loss after iteration 2500: 0.398653\n",
            "Loss after iteration 2600: 0.391355\n",
            "Loss after iteration 2700: 0.384535\n",
            "Loss after iteration 2800: 0.378139\n",
            "Loss after iteration 2900: 0.372120\n",
            "Loss after iteration 3000: 0.366445\n",
            "Loss after iteration 3100: 0.361088\n",
            "Loss after iteration 3200: 0.356011\n",
            "Loss after iteration 3300: 0.351189\n",
            "Loss after iteration 3400: 0.346595\n",
            "Loss after iteration 3500: 0.342203\n",
            "Loss after iteration 3600: 0.338006\n",
            "Loss after iteration 3700: 0.333999\n",
            "Loss after iteration 3800: 0.330163\n",
            "Loss after iteration 3900: 0.326477\n",
            "Loss after iteration 4000: 0.322936\n",
            "Loss after iteration 4100: 0.319525\n",
            "Loss after iteration 4200: 0.316231\n",
            "Loss after iteration 4300: 0.313051\n",
            "Loss after iteration 4400: 0.309978\n",
            "Loss after iteration 4500: 0.307007\n",
            "Loss after iteration 4600: 0.304133\n",
            "Loss after iteration 4700: 0.301345\n",
            "Loss after iteration 4800: 0.298635\n",
            "Loss after iteration 4900: 0.295998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ygL-ctIk3d7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bd9fe90-a68d-4b77-f5a2-fee814f2d6ff"
      },
      "source": [
        "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9208000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqafRIgDk3kK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "71cd5212-0403-4363-fe21-02c17aa9d4d8"
      },
      "source": [
        "pred_test = predict(test_set_x, test_set_y, parameters)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8931000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgawJ-ysk3tP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "e4bbc93f-05b6-45e3-984b-23034678c113"
      },
      "source": [
        "index  = 3474\n",
        "k = test_set_x[:,index]\n",
        "k = k.reshape((28, 28))\n",
        "plt.title('Label is {label}'.format(label=(pred_test[index], np.argmax(test_set_y, axis = 0)[index])))\n",
        "plt.imshow(k, cmap='gray')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fabd7f38a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Q-3SujlIat",
        "colab_type": "text"
      },
      "source": [
        "## **Training for 8000 iterations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drD5wrM_lOw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2cba5028-37ee-48f4-f4c9-270642e7e8bd"
      },
      "source": [
        "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 8000, print_loss = True)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 2.422624\n",
            "Loss after iteration 100: 2.129232\n",
            "Loss after iteration 200: 1.876095\n",
            "Loss after iteration 300: 1.604213\n",
            "Loss after iteration 400: 1.350205\n",
            "Loss after iteration 500: 1.144823\n",
            "Loss after iteration 600: 0.990554\n",
            "Loss after iteration 700: 0.876603\n",
            "Loss after iteration 800: 0.791154\n",
            "Loss after iteration 900: 0.725441\n",
            "Loss after iteration 1000: 0.673485\n",
            "Loss after iteration 1100: 0.631386\n",
            "Loss after iteration 1200: 0.596598\n",
            "Loss after iteration 1300: 0.567342\n",
            "Loss after iteration 1400: 0.542346\n",
            "Loss after iteration 1500: 0.520746\n",
            "Loss after iteration 1600: 0.501865\n",
            "Loss after iteration 1700: 0.485205\n",
            "Loss after iteration 1800: 0.470368\n",
            "Loss after iteration 1900: 0.457054\n",
            "Loss after iteration 2000: 0.445034\n",
            "Loss after iteration 2100: 0.434120\n",
            "Loss after iteration 2200: 0.424141\n",
            "Loss after iteration 2300: 0.414967\n",
            "Loss after iteration 2400: 0.406498\n",
            "Loss after iteration 2500: 0.398653\n",
            "Loss after iteration 2600: 0.391355\n",
            "Loss after iteration 2700: 0.384535\n",
            "Loss after iteration 2800: 0.378139\n",
            "Loss after iteration 2900: 0.372120\n",
            "Loss after iteration 3000: 0.366445\n",
            "Loss after iteration 3100: 0.361088\n",
            "Loss after iteration 3200: 0.356011\n",
            "Loss after iteration 3300: 0.351189\n",
            "Loss after iteration 3400: 0.346595\n",
            "Loss after iteration 3500: 0.342203\n",
            "Loss after iteration 3600: 0.338006\n",
            "Loss after iteration 3700: 0.333999\n",
            "Loss after iteration 3800: 0.330163\n",
            "Loss after iteration 3900: 0.326477\n",
            "Loss after iteration 4000: 0.322936\n",
            "Loss after iteration 4100: 0.319525\n",
            "Loss after iteration 4200: 0.316231\n",
            "Loss after iteration 4300: 0.313051\n",
            "Loss after iteration 4400: 0.309978\n",
            "Loss after iteration 4500: 0.307007\n",
            "Loss after iteration 4600: 0.304133\n",
            "Loss after iteration 4700: 0.301345\n",
            "Loss after iteration 4800: 0.298635\n",
            "Loss after iteration 4900: 0.295998\n",
            "Loss after iteration 5000: 0.293435\n",
            "Loss after iteration 5100: 0.290943\n",
            "Loss after iteration 5200: 0.288515\n",
            "Loss after iteration 5300: 0.286148\n",
            "Loss after iteration 5400: 0.283838\n",
            "Loss after iteration 5500: 0.281582\n",
            "Loss after iteration 5600: 0.279382\n",
            "Loss after iteration 5700: 0.277234\n",
            "Loss after iteration 5800: 0.275133\n",
            "Loss after iteration 5900: 0.273071\n",
            "Loss after iteration 6000: 0.271047\n",
            "Loss after iteration 6100: 0.269061\n",
            "Loss after iteration 6200: 0.267115\n",
            "Loss after iteration 6300: 0.265206\n",
            "Loss after iteration 6400: 0.263333\n",
            "Loss after iteration 6500: 0.261500\n",
            "Loss after iteration 6600: 0.259700\n",
            "Loss after iteration 6700: 0.257930\n",
            "Loss after iteration 6800: 0.256190\n",
            "Loss after iteration 6900: 0.254480\n",
            "Loss after iteration 7000: 0.252798\n",
            "Loss after iteration 7100: 0.251140\n",
            "Loss after iteration 7200: 0.249506\n",
            "Loss after iteration 7300: 0.247897\n",
            "Loss after iteration 7400: 0.246311\n",
            "Loss after iteration 7500: 0.244751\n",
            "Loss after iteration 7600: 0.243212\n",
            "Loss after iteration 7700: 0.241695\n",
            "Loss after iteration 7800: 0.240199\n",
            "Loss after iteration 7900: 0.238715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbZj9BynlPQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06ffab22-ab46-4042-86d1-4e72aa407a79"
      },
      "source": [
        "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9362000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jpDfloslPU7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3badc32-bfc1-413d-b9b0-3718790fb458"
      },
      "source": [
        "pred_test = predict(test_set_x, test_set_y, parameters)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9021000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDzMXLhtlPl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzvVRsKrlndI",
        "colab_type": "text"
      },
      "source": [
        "## **Training for 3 Hidden layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfsyf_3alwtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dimensions=[784,54,60,20,10]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbxwZ1ktlwzY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e230a1b8-6b82-4053-a3a8-e1276dc0ad21"
      },
      "source": [
        "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 5000, print_loss = True)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 2.308640\n",
            "Loss after iteration 100: 2.297980\n",
            "Loss after iteration 200: 2.288271\n",
            "Loss after iteration 300: 2.276689\n",
            "Loss after iteration 400: 2.261130\n",
            "Loss after iteration 500: 2.238922\n",
            "Loss after iteration 600: 2.205851\n",
            "Loss after iteration 700: 2.155171\n",
            "Loss after iteration 800: 2.076893\n",
            "Loss after iteration 900: 1.959354\n",
            "Loss after iteration 1000: 1.794742\n",
            "Loss after iteration 1100: 1.597779\n",
            "Loss after iteration 1200: 1.405575\n",
            "Loss after iteration 1300: 1.241166\n",
            "Loss after iteration 1400: 1.107877\n",
            "Loss after iteration 1500: 1.001816\n",
            "Loss after iteration 1600: 0.917593\n",
            "Loss after iteration 1700: 0.850142\n",
            "Loss after iteration 1800: 0.795190\n",
            "Loss after iteration 1900: 0.749008\n",
            "Loss after iteration 2000: 0.709309\n",
            "Loss after iteration 2100: 0.674259\n",
            "Loss after iteration 2200: 0.642899\n",
            "Loss after iteration 2300: 0.614534\n",
            "Loss after iteration 2400: 0.588672\n",
            "Loss after iteration 2500: 0.564951\n",
            "Loss after iteration 2600: 0.543101\n",
            "Loss after iteration 2700: 0.522942\n",
            "Loss after iteration 2800: 0.504290\n",
            "Loss after iteration 2900: 0.487069\n",
            "Loss after iteration 3000: 0.471118\n",
            "Loss after iteration 3100: 0.456369\n",
            "Loss after iteration 3200: 0.442709\n",
            "Loss after iteration 3300: 0.430069\n",
            "Loss after iteration 3400: 0.418324\n",
            "Loss after iteration 3500: 0.407388\n",
            "Loss after iteration 3600: 0.397154\n",
            "Loss after iteration 3700: 0.387540\n",
            "Loss after iteration 3800: 0.378486\n",
            "Loss after iteration 3900: 0.369900\n",
            "Loss after iteration 4000: 0.361759\n",
            "Loss after iteration 4100: 0.354035\n",
            "Loss after iteration 4200: 0.346652\n",
            "Loss after iteration 4300: 0.339558\n",
            "Loss after iteration 4400: 0.332742\n",
            "Loss after iteration 4500: 0.326194\n",
            "Loss after iteration 4600: 0.319920\n",
            "Loss after iteration 4700: 0.313858\n",
            "Loss after iteration 4800: 0.308003\n",
            "Loss after iteration 4900: 0.302343\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1i9E9oelxMo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "384fa1a5-d0ac-4ced-8d86-614ce31a6277"
      },
      "source": [
        "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9180000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvFcwTpXlwqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "140e8baa-d30b-4563-a8e6-8f7f3fb515c3"
      },
      "source": [
        "pred_test = predict(test_set_x, test_set_y, parameters)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8825000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUT0MxlHmGM3",
        "colab_type": "text"
      },
      "source": [
        "## **Training for 1 Hidden layers and 100 neurons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AENG2gSwmM6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dimensions=[784,100,10]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWJcKNf5mM_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd1b5e26-2249-4841-949f-5413771563d8"
      },
      "source": [
        "parameters = L_layer_model(train_set_x_new, train_set_y_new, dimensions, num_iterations = 5000, print_loss = True)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 2.664801\n",
            "Loss after iteration 100: 1.967476\n",
            "Loss after iteration 200: 1.596075\n",
            "Loss after iteration 300: 1.295950\n",
            "Loss after iteration 400: 1.075244\n",
            "Loss after iteration 500: 0.919353\n",
            "Loss after iteration 600: 0.808084\n",
            "Loss after iteration 700: 0.726452\n",
            "Loss after iteration 800: 0.664388\n",
            "Loss after iteration 900: 0.615790\n",
            "Loss after iteration 1000: 0.576722\n",
            "Loss after iteration 1100: 0.544622\n",
            "Loss after iteration 1200: 0.517726\n",
            "Loss after iteration 1300: 0.494853\n",
            "Loss after iteration 1400: 0.475131\n",
            "Loss after iteration 1500: 0.457917\n",
            "Loss after iteration 1600: 0.442717\n",
            "Loss after iteration 1700: 0.429187\n",
            "Loss after iteration 1800: 0.417054\n",
            "Loss after iteration 1900: 0.406104\n",
            "Loss after iteration 2000: 0.396129\n",
            "Loss after iteration 2100: 0.386989\n",
            "Loss after iteration 2200: 0.378584\n",
            "Loss after iteration 2300: 0.370805\n",
            "Loss after iteration 2400: 0.363555\n",
            "Loss after iteration 2500: 0.356789\n",
            "Loss after iteration 2600: 0.350442\n",
            "Loss after iteration 2700: 0.344467\n",
            "Loss after iteration 2800: 0.338834\n",
            "Loss after iteration 2900: 0.333505\n",
            "Loss after iteration 3000: 0.328453\n",
            "Loss after iteration 3100: 0.323657\n",
            "Loss after iteration 3200: 0.319089\n",
            "Loss after iteration 3300: 0.314726\n",
            "Loss after iteration 3400: 0.310551\n",
            "Loss after iteration 3500: 0.306552\n",
            "Loss after iteration 3600: 0.302718\n",
            "Loss after iteration 3700: 0.299037\n",
            "Loss after iteration 3800: 0.295483\n",
            "Loss after iteration 3900: 0.292054\n",
            "Loss after iteration 4000: 0.288737\n",
            "Loss after iteration 4100: 0.285528\n",
            "Loss after iteration 4200: 0.282419\n",
            "Loss after iteration 4300: 0.279405\n",
            "Loss after iteration 4400: 0.276486\n",
            "Loss after iteration 4500: 0.273654\n",
            "Loss after iteration 4600: 0.270900\n",
            "Loss after iteration 4700: 0.268221\n",
            "Loss after iteration 4800: 0.265610\n",
            "Loss after iteration 4900: 0.263062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c9ohJOWmNNE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6665496e-0da9-445b-a1ea-3e6d5ffcc84d"
      },
      "source": [
        "pred_train = predict(train_set_x_new, train_set_y_new, parameters)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9284000000000002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTW8XVtpmNYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "275756f9-f73f-4565-fc97-fef4a50f1361"
      },
      "source": [
        "pred_test = predict(test_set_x, test_set_y, parameters)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9006000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Dnu9F8mNUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff7qlbCEmNKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18NXNDYvmNHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}